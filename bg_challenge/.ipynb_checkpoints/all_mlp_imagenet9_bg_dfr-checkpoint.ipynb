{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate DFR on ImageNet-9 variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import einops\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\")\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 2000\n",
    "\n",
    "ns = [1000, 5000, 10000, 20000, 45405]\n",
    "num_seeds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_to_in9 = {}\n",
    "with open('in_to_in9.json', 'r') as f:\n",
    "    map_to_in9.update(json.load(f))\n",
    "\n",
    "def load_embeddings(path):\n",
    "    arr = np.load(path)\n",
    "    x, y = arr[\"embeddings\"], arr[\"labels\"]\n",
    "    if np.max(y) > 9:\n",
    "        y = np.array([map_to_in9[str(y_)] for y_ in y])\n",
    "        mask = y != -1\n",
    "        x, y = x[mask], y[mask]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(\n",
    "    x_train, y_train, eval_datasets,\n",
    "    n_epochs=1000, weight_decay=0., lr=1.,\n",
    "    verbose=0\n",
    "    ):\n",
    "    \n",
    "    x_train = torch.from_numpy(x_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).long().to(device)\n",
    "    \n",
    "    d = x_train.shape[1]\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(d, 2048),  \n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(2048, 9)  \n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "    schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        schedule.step()\n",
    "        acc = (torch.argmax(pred, -1) == y_train).detach().float().mean()\n",
    "        if verbose > 1 and epoch % (n_epochs // 10) == 0:\n",
    "            print(epoch, acc)\n",
    "    \n",
    "    results = {}\n",
    "    for key, (x_test, y_test) in eval_datasets.items():\n",
    "        x_test = torch.from_numpy(x_test).float().to(device)\n",
    "        pred = torch.argmax(model(x_test), axis=-1).detach().cpu().numpy()\n",
    "        results[key] = (pred == y_test).mean()\n",
    "    \n",
    "    pred = torch.argmax(model(x_train), axis=-1)\n",
    "    results[\"train\"] = (pred == y_train).detach().cpu().float().mean().item()\n",
    "\n",
    "    return model, results\n",
    "\n",
    "def get_data(\n",
    "    train_datasets, eval_datasets, \n",
    "    num_mixrand=-1, num_original=0, preprocess=True):\n",
    "    \n",
    "    x_train, y_train = train_datasets[\"bgc_original\"]\n",
    "    idx = np.arange(len(x_train))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num_original]\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "\n",
    "    x_train_mr, y_train_mr = train_datasets[\"bgc_mixedrand\"]\n",
    "    idx = np.arange(len(x_train_mr))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num_mixrand]\n",
    "    x_train_mr = x_train_mr[idx]\n",
    "    y_train_mr = y_train_mr[idx]\n",
    "\n",
    "    x_train = np.concatenate([x_train, x_train_mr])\n",
    "    y_train = np.concatenate([y_train, y_train_mr])\n",
    "\n",
    "    if preprocess:\n",
    "        scaler = StandardScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        eval_datasets_preprocessed = {\n",
    "            k: (scaler.transform(x), y)\n",
    "            for k, (x, y) in eval_datasets.items()\n",
    "        }\n",
    "    else:\n",
    "        eval_datasets_preprocessed = eval_datasets\n",
    "    return x_train, y_train, eval_datasets_preprocessed\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    train_datasets, eval_datasets,\n",
    "    num_mixrand=-1, num_original=0, preprocess=True,\n",
    "    n_epochs=1000, weight_decay=0., lr=1., verbose=0,\n",
    "    num_seeds=3\n",
    "):\n",
    "    print(lr)\n",
    "    print(weight_decay)\n",
    "    print(n_epochs)\n",
    "    results = {}\n",
    "    for seed in range(num_seeds):\n",
    "        x_train, y_train, eval_datasets_preprocessed = get_data(\n",
    "            train_datasets, eval_datasets,\n",
    "            num_mixrand, num_original, preprocess)\n",
    "        _, results_seed = train_logreg(\n",
    "            x_train, y_train, eval_datasets_preprocessed,\n",
    "            n_epochs, weight_decay, lr, verbose)\n",
    "        results[seed] = results_seed\n",
    "    \n",
    "    results_aggrgated = {\n",
    "        key: (np.mean([results[seed][key] for seed in results.keys()]),\n",
    "              np.std([results[seed][key] for seed in results.keys()]))\n",
    "        for key in results[0].keys()\n",
    "    }\n",
    "    return results, results_aggrgated\n",
    "\n",
    "\n",
    "def print_results(results_dict):\n",
    "    print(\"-------------------\")\n",
    "    for key, val in results_dict.items():\n",
    "        print(\"{}: {:.3f}Â±{:.3f}\".format(key, val[0], val[1]))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixrand_run(ns, n_epochs, lr, train_datasets, eval_datasets, num_seeds):\n",
    "    mixrand_results_ns = []\n",
    "    for n in ns:\n",
    "        num_mixrand = n\n",
    "        num_original = 0\n",
    "\n",
    "        n_data = num_mixrand + num_original\n",
    "        wd = 100 / n_data\n",
    "        seed_results, results_aggrgated = run_experiment(\n",
    "            train_datasets, eval_datasets,\n",
    "            num_mixrand, num_original, lr=lr, weight_decay=wd, n_epochs=n_epochs,\n",
    "            num_seeds=num_seeds)\n",
    "        print_results(results_aggrgated)\n",
    "        mixrand_results_ns.append(results_aggrgated)\n",
    "    return mixrand_results_ns\n",
    "\n",
    "\n",
    "def combo_run(ns, n_epochs, lr, train_datasets, eval_datasets, num_seeds):\n",
    "    combo_results_ns = []\n",
    "    for n in ns:\n",
    "        num_mixrand = n\n",
    "        num_original = n\n",
    "\n",
    "        n_data = num_mixrand + num_original\n",
    "        wd = 100 / n_data\n",
    "        seed_results, results_aggrgated = run_experiment(\n",
    "            train_datasets, eval_datasets,\n",
    "            num_mixrand, num_original, lr=lr, weight_decay=wd, n_epochs=n_epochs,\n",
    "            num_seeds=num_seeds)\n",
    "        print_results(results_aggrgated)\n",
    "        combo_results_ns.append(results_aggrgated)\n",
    "    return combo_results_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_indv(key, ax, ns, mixrand_results_ns, combo_results_ns, results_original):\n",
    "    mixrand_mu = np.array([results[key][0] for results in mixrand_results_ns])\n",
    "    mixrand_sigma = np.array([results[key][1] for results in mixrand_results_ns])\n",
    "\n",
    "    original_mu = np.array([results_original[key][0] for _ in ns])\n",
    "    original_sigma = np.array([results_original[key][1] for _ in ns])\n",
    "\n",
    "    combo_mu = np.array([results[key][0] for results in combo_results_ns])\n",
    "    combo_sigma = np.array([results[key][1] for results in combo_results_ns])\n",
    "    ax.plot(ns, mixrand_mu, \"-bo\", label=\"DFR MR\")\n",
    "    ax.fill_between(ns, mixrand_mu + mixrand_sigma, mixrand_mu - mixrand_sigma,\n",
    "                   color=\"b\", alpha=0.3)\n",
    "    ax.plot(ns, combo_mu, \"-ro\", label=\"DFR MR + Og\")\n",
    "    ax.fill_between(ns, combo_mu + combo_sigma, combo_mu - combo_sigma,\n",
    "                    color=\"r\", alpha=0.3)\n",
    "    ax.plot(ns, original_mu, \"--g\", label=\"DFR Og\")\n",
    "    ax.fill_between(ns, original_mu + original_sigma, original_mu - original_sigma,\n",
    "                    color=\"g\", alpha=0.3)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"# MixedRand Data\", fontsize=12)\n",
    "    ax.set_ylabel(f\"{key} Test Acc\", fontsize=12)\n",
    "\n",
    "def plot_all_results_indv(mixrand_results_ns, combo_results_ns, results_original):\n",
    "    f, arr = plt.subplots(1, 5, figsize=(16, 3))\n",
    "\n",
    "    plot_results_indv(\"bgc_mixedrand\", arr[0], np.array(ns), mixrand_results_ns, combo_results_ns, results_original)\n",
    "    plot_results_indv(\"bgc_fg\", arr[1], np.array(ns), mixrand_results_ns, combo_results_ns, results_original)\n",
    "    plot_results_indv(\"bgc_original\", arr[2], np.array(ns), mixrand_results_ns, combo_results_ns, results_original)\n",
    "    plot_results_indv(\"imagenet_r\", arr[3], np.array(ns), mixrand_results_ns, combo_results_ns, results_original)\n",
    "    plot_results_indv(\"imagenet_a\", arr[4], np.array(ns), mixrand_results_ns, combo_results_ns, results_original)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_prev_layer = False\n",
    "\n",
    "eval_path_dict = {\n",
    "    \"imagenet_r\": f\"/home/jennyni/datasets/imagenet-r/imagenet-r_{use_prev_layer}_val_embeddings.npz\",\n",
    "    \"imagenet_a\": f\"/home/jennyni/datasets/imagenet-a/imagenet-a_{use_prev_layer}_val_embeddings.npz\",\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/bg_challenge/original/bg_challenge_{use_prev_layer}_val_embeddings.npz\",\n",
    "    \"bgc_fg\": f\"/home/jennyni/datasets/bg_challenge/only_fg/bg_challenge_{use_prev_layer}_val_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/bg_challenge/mixed_rand/bg_challenge_{use_prev_layer}_val_embeddings.npz\",\n",
    "}\n",
    "\n",
    "eval_datasets = {k: load_embeddings(p) for k, p in eval_path_dict.items()}\n",
    "\n",
    "train_path_dict = {\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/original/bg_challenge_{use_prev_layer}_train_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/mixed_rand/bg_challenge_{use_prev_layer}_train_embeddings.npz\",\n",
    "\n",
    "}\n",
    "train_datasets = {k: load_embeddings(p) for k, p in train_path_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.0022024006166721727\n",
      "2000\n",
      "-------------------\n",
      "imagenet_r: 0.551Â±0.000\n",
      "imagenet_a: 0.372Â±0.003\n",
      "bgc_original: 0.980Â±0.000\n",
      "bgc_fg: 0.930Â±0.001\n",
      "bgc_mixedrand: 0.866Â±0.001\n",
      "train: 0.996Â±0.000\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "_, results_original_ori = run_experiment(train_datasets, eval_datasets,\n",
    "                                                num_mixrand=0, num_original=-1, lr=learning_rate, num_seeds=num_seeds, n_epochs=num_epochs,\n",
    "                                                weight_decay=100/45405)\n",
    "\n",
    "print_results(results_original_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.1\n",
      "2000\n",
      "-------------------\n",
      "imagenet_r: 0.496Â±0.016\n",
      "imagenet_a: 0.289Â±0.011\n",
      "bgc_original: 0.943Â±0.003\n",
      "bgc_fg: 0.900Â±0.006\n",
      "bgc_mixedrand: 0.873Â±0.004\n",
      "train: 0.995Â±0.002\n",
      "-------------------\n",
      "0.1\n",
      "0.02\n",
      "2000\n",
      "-------------------\n",
      "imagenet_r: 0.536Â±0.012\n",
      "imagenet_a: 0.286Â±0.004\n",
      "bgc_original: 0.957Â±0.003\n",
      "bgc_fg: 0.921Â±0.003\n",
      "bgc_mixedrand: 0.906Â±0.002\n",
      "train: 0.998Â±0.000\n",
      "-------------------\n",
      "0.1\n",
      "0.01\n",
      "2000\n",
      "-------------------\n",
      "imagenet_r: 0.545Â±0.003\n",
      "imagenet_a: 0.286Â±0.005\n",
      "bgc_original: 0.961Â±0.002\n",
      "bgc_fg: 0.935Â±0.002\n",
      "bgc_mixedrand: 0.917Â±0.003\n",
      "train: 0.998Â±0.000\n",
      "-------------------\n",
      "0.1\n",
      "0.005\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "mixrand_results_ns_ori = mixrand_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)\n",
    "combo_results_ns_ori = combo_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(key, ax, ns, mixrand_results_ns, combo_results_ns, results_original):\n",
    "    mixrand_mu = np.array([results[key][0] for results in mixrand_results_ns])\n",
    "    mixrand_sigma = np.array([results[key][1] for results in mixrand_results_ns])\n",
    "\n",
    "    original_mu = np.array([results_original[key][0] for _ in ns])\n",
    "    original_sigma = np.array([results_original[key][1] for _ in ns])\n",
    "\n",
    "    combo_mu = np.array([results[key][0] for results in combo_results_ns])\n",
    "    combo_sigma = np.array([results[key][1] for results in combo_results_ns])\n",
    "    ax.plot(ns, mixrand_mu, \"-bo\", label=\"DFR MR\")\n",
    "    ax.fill_between(ns, mixrand_mu + mixrand_sigma, mixrand_mu - mixrand_sigma,\n",
    "                   color=\"b\", alpha=0.3)\n",
    "    ax.plot(ns, combo_mu, \"-ro\", label=\"DFR MR + Og\")\n",
    "    ax.fill_between(ns, combo_mu + combo_sigma, combo_mu - combo_sigma,\n",
    "                    color=\"r\", alpha=0.3)\n",
    "    ax.plot(ns, original_mu, \"--g\", label=\"DFR Og\")\n",
    "    ax.fill_between(ns, original_mu + original_sigma, original_mu - original_sigma,\n",
    "                    color=\"g\", alpha=0.3)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"# MixedRand Data\", fontsize=12)\n",
    "    ax.set_ylabel(f\"{key} Test Acc\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_original_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results_indv(mixrand_results_ns_ori, combo_results_ns_ori, results_original_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ph = False\n",
    "\n",
    "eval_path_dict = {\n",
    "    \"imagenet_r\": f\"/home/jennyni/datasets/imagenet-r/new_ph_imagenet-r_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"imagenet_a\": f\"/home/jennyni/datasets/imagenet-a/new_ph_imagenet-a_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/bg_challenge/original/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_fg\": f\"/home/jennyni/datasets/bg_challenge/only_fg/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/bg_challenge/mixed_rand/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "}\n",
    "\n",
    "eval_datasets = {k: load_embeddings(p) for k, p in eval_path_dict.items()}\n",
    "\n",
    "train_path_dict = {\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/original/new_ph_bg_challenge_{use_ph}_train_standardSL_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/mixed_rand/new_ph_bg_challenge_{use_ph}_train_standardSL_embeddings.npz\",\n",
    "}\n",
    "train_datasets = {k: load_embeddings(p) for k, p in train_path_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results_original_pre = run_experiment(train_datasets, eval_datasets,\n",
    "                                                num_mixrand=0, num_original=-1, lr=learning_rate, num_seeds=num_seeds, n_epochs=num_epochs,\n",
    "                                                weight_decay=100/45405)\n",
    "\n",
    "print_results(results_original_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mixrand_results_ns_pre = mixrand_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)\n",
    "combo_results_ns_pre = combo_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results_indv(mixrand_results_ns_pre, combo_results_ns_pre, results_original_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ph = True\n",
    "\n",
    "eval_path_dict = {\n",
    "    \"imagenet_r\": f\"/home/jennyni/datasets/imagenet-r/new_ph_imagenet-r_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"imagenet_a\": f\"/home/jennyni/datasets/imagenet-a/new_ph_imagenet-a_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/bg_challenge/original/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_fg\": f\"/home/jennyni/datasets/bg_challenge/only_fg/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/bg_challenge/mixed_rand/new_ph_bg_challenge_{use_ph}_val_standardSL_embeddings.npz\",\n",
    "}\n",
    "\n",
    "eval_datasets = {k: load_embeddings(p) for k, p in eval_path_dict.items()}\n",
    "\n",
    "train_path_dict = {\n",
    "    \"bgc_original\": f\"/home/jennyni/datasets/original/new_ph_bg_challenge_{use_ph}_train_standardSL_embeddings.npz\",\n",
    "    \"bgc_mixedrand\": f\"/home/jennyni/datasets/mixed_rand/new_ph_bg_challenge_{use_ph}_train_standardSL_embeddings.npz\",\n",
    "}\n",
    "train_datasets = {k: load_embeddings(p) for k, p in train_path_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, results_original_post = run_experiment(train_datasets, eval_datasets,\n",
    "                                                num_mixrand=0, num_original=-1, lr=learning_rate, num_seeds=num_seeds, n_epochs=num_epochs,\n",
    "                                                weight_decay=100/45405)\n",
    "\n",
    "print_results(results_original_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mixrand_results_ns_post = mixrand_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)\n",
    "combo_results_ns_post = combo_run(ns, n_epochs=num_epochs, lr=learning_rate, train_datasets=train_datasets, eval_datasets=eval_datasets, num_seeds=num_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results_indv(mixrand_results_ns_post, combo_results_ns_post, results_original_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Distribution Accuracy vs In Distribution Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def process_results(results):\n",
    "    in_distrib_acc = []\n",
    "    out_distrib_acc = []\n",
    "    in_distrib_std = []\n",
    "    out_distrib_std = []\n",
    "    \n",
    "    for res in results:\n",
    "        in_distrib_acc.append(res['bgc_original'][0])\n",
    "        in_distrib_std.append(res['bgc_original'][1])\n",
    "        \n",
    "        # out_distrib_means = [v[0] for k, v in res.items() if k != 'bgc_original' and k != 'train' and k != 'imagenet_a']\n",
    "        # out_distrib_stds = [v[1] for k, v in res.items() if k != 'bgc_original' and k != 'train' and k != 'imagenet_a']\n",
    "        out_distrib_means = [v[0] for k, v in res.items() if k != 'bgc_original' and k != 'train']\n",
    "        out_distrib_stds = [v[1] for k, v in res.items() if k != 'bgc_original' and k != 'train']\n",
    "        \n",
    "        out_distrib_acc.append(np.mean(out_distrib_means))\n",
    "        out_distrib_std.append(np.sqrt(np.mean(np.array(out_distrib_stds) ** 2)))  # Pooled standard deviation\n",
    "        \n",
    "    return in_distrib_acc, out_distrib_acc, in_distrib_std, out_distrib_std\n",
    "\n",
    "\n",
    "def plot_results(in_acc, out_acc, in_std, out_std, label, color, n=5, ax=None):\n",
    "    in_err = np.array(in_std) / np.sqrt(n)\n",
    "    out_err = np.array(out_std) / np.sqrt(n)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca() \n",
    "\n",
    "    ax.errorbar(in_acc, out_acc, xerr=in_err, yerr=out_err, fmt='o', label=label, color=color)\n",
    "    slope, intercept, _, _, _ = stats.linregress(in_acc, out_acc)\n",
    "    in_acc_line = np.array(in_acc)\n",
    "    out_acc_line = slope * in_acc_line + intercept\n",
    "\n",
    "    ax.plot(in_acc_line, out_acc_line, '--', lw=2, color=color)\n",
    "    ax.fill_between(in_acc_line, out_acc_line - out_err, out_acc_line + out_err, alpha=0.2, color=color)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))  \n",
    "\n",
    "in_original, out_original, in_std_original, out_std_original = process_results(mixrand_results_ns_ori)\n",
    "in_pre, out_pre, in_std_pre, out_std_pre = process_results(mixrand_results_ns_pre)\n",
    "in_post, out_post, in_std_post, out_std_post = process_results(mixrand_results_ns_post)\n",
    "axs[0].set_title('MR In vs Out-Of-Distribution Accuracy')\n",
    "plot_results(in_original, out_original, in_std_original, out_std_original, 'Original', 'blue', ax=axs[0])\n",
    "plot_results(in_pre, out_pre, in_std_pre, out_std_pre, 'Pre', 'green', ax=axs[0])\n",
    "plot_results(in_post, out_post, in_std_post, out_std_post, 'Post', 'red', ax=axs[0])\n",
    "\n",
    "\n",
    "in_original, out_original, in_std_original, out_std_original = process_results(combo_results_ns_ori)\n",
    "in_pre, out_pre, in_std_pre, out_std_pre = process_results(combo_results_ns_pre)\n",
    "in_post, out_post, in_std_post, out_std_post = process_results(combo_results_ns_post)\n",
    "\n",
    "axs[1].set_title('MR + OG In vs Out-Of-Distribution Accuracy')\n",
    "plot_results(in_original, out_original, in_std_original, out_std_original, 'Original', 'blue', ax=axs[1])\n",
    "plot_results(in_pre, out_pre, in_std_pre, out_std_pre, 'Pre', 'green', ax=axs[1])\n",
    "plot_results(in_post, out_post, in_std_post, out_std_post, 'Post', 'red', ax=axs[1])\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('In-Distribution Accuracy')\n",
    "    ax.set_ylabel('Out-Of-Distribution Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"projection_head_mlp.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"MR_results_ori:\\n\")\n",
    "    file.write(str(mixrand_results_ns_ori) + \"\\n\\n\")\n",
    "    \n",
    "    file.write(\"MR_results_pre:\\n\")\n",
    "    file.write(str(mixrand_results_ns_pre) + \"\\n\\n\")\n",
    "    \n",
    "    file.write(\"MR_results_post:\\n\")\n",
    "    file.write(str(mixrand_results_ns_post) + \"\\n\\n\")\n",
    "    \n",
    "    file.write(\"MROG_results_ori:\\n\")\n",
    "    file.write(str(combo_results_ns_ori) + \"\\n\\n\")\n",
    "    \n",
    "    file.write(\"MROG_results_pre:\\n\")\n",
    "    file.write(str(combo_results_ns_pre) + \"\\n\\n\")\n",
    "    \n",
    "    file.write(\"MROG_results_post:\\n\")\n",
    "    file.write(str(combo_results_ns_post) + \"\\n\\n\")\n",
    "\n",
    "print(f\"Results saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
